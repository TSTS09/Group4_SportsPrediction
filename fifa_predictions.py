# -*- coding: utf-8 -*-
"""FIFA_predictions

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZihLIcCnR8id2FJow8YmWloCVv368yMB
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.impute import KNNImputer, SimpleImputer
from sklearn.ensemble import RandomForestClassifier
#to ignore warnings
import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

#Loading the datasets to be used (both training and testing)
training_data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/A 2023/players_21.csv')
data1 = training_data.copy()

"""# 1. Data Pre-processing and Feature extraction process

### Removing useless variables (Those with 30% missing values)

Removed columns containing URLs and players specific physical information since they do not affect the overall rating of the players but create an unexplained variance in the model, reducing the accuracy of the feature selection process
"""

first_variables = training_data[['sofifa_id','overall', 'potential', 'age','weak_foot','skill_moves','international_reputation','work_rate','body_type']]
second_variables = training_data.loc[:, 'pace':'goalkeeping_speed']
training_data = pd.concat([first_variables,second_variables], axis=1)

"""Remove variables with more than 30% missing values (useless variables)"""

threshold = 0.30 *len(training_data)

training_data = training_data.dropna(thresh=threshold, axis =1)

"""#Exploratory Data Analysis(EDA)

Carrying out  exploratory data analysis (EDA) to better understand the data
"""

#Viewing the first five element of the training data
training_data.head()

#Viewing the last five element of the training data
training_data.tail()

training_data.info()

training_data.nunique() #Checking for the number of duplications in the columns

training_data.isnull().sum() # Getting the number of missing values in each columns

((training_data.isnull().sum())/len(training_data))*100 # percentage of missing values in each column

training_data.describe(include='all').T #Understanding the statistical description of the data

training_data.dtypes   #viewing the data types of the columns

"""### Understanding how strongly different variables (columns in the DataFrame) are related to each other."""

correlation_matrix= training_data.corr()

sns.heatmap(correlation_matrix)

#Displays the summary of the set of data values of the overall column
training_data[['overall']].boxplot()

#Generating a plot that shows the frequency or count of each unique value in the 'overall' column
sns.countplot(x=training_data['overall'])

""" Performing a t-test to assess whether there is a statistically significant difference between the 'overall' and 'potential' data in the training_data. The t-statistic of -80.11 indicates a substantial difference between the groups being compared. The p-value of 0.0 suggests that this difference is highly unlikely to have occurred by chance, leading to the rejection of the null hypothesis and indicating a significant difference between the groups."""

from scipy.stats import ttest_ind

group_1 = training_data['overall']
group_2 = training_data['potential']

t_stat, p_value = ttest_ind(group_1, group_2)

print(f"t-statistic: {t_stat}")
rounded_p_value = round(p_value, 5)
print(f"P-value: {rounded_p_value}")

if rounded_p_value < 0.05:
    print("Reject the null hypothesis (significant difference between groups).")
else:
    print("Fail to reject the null hypothesis (no significant difference between groups).")

"""### Splitting numerical and categorical variables"""

y = training_data['overall']
training_data.drop('overall', axis=1,inplace=True)

"""Separating the numerical and categorical variables"""

numerical_columns = training_data.select_dtypes(include=['int64', 'float64']).columns
categorical_columns = training_data.select_dtypes(include=['object']).columns

categorical_var = training_data[categorical_columns]
numerical_var = training_data[numerical_columns]

numerical_var

categorical_var

"""### KNN imputation of numerical variables"""

knn_imputer = KNNImputer(n_neighbors=8)
numerical_var_imputed = knn_imputer.fit_transform(numerical_var)
numerical_columns = list(numerical_var.columns)
numerical_var_imputed = pd.DataFrame(numerical_var_imputed, columns=numerical_columns)

"""### Imputation and Encoding categorical variables

Imputing the the NaN of the catagorical variabe with simple imputer using the most frequent elements in the columns and encoding the categorical variable with factorize function
"""

categorical_imputer = SimpleImputer(strategy='most_frequent')
categorical_var_imputed = categorical_imputer.fit_transform(categorical_var)

categorical_columns = list(categorical_var.columns)
Categorical_var_2 = pd.DataFrame(categorical_var_imputed, columns = categorical_columns)

columns_to_encode = categorical_var.loc[:, 'work_rate':'body_type']

categorical_var[columns_to_encode.columns] = columns_to_encode.apply(lambda x: pd.factorize(x)[0])

Categorical_var_2= pd.DataFrame(categorical_var[columns_to_encode.columns],)

Categorical_var_2

"""### Joining back the preprocessed numerical and categorical datasets to form a complete dataset with relevant columns"""

clean_data = pd.concat([numerical_var_imputed, Categorical_var_2], axis=1)
clean_data

"""# 2. Feature Engineering

#Determining feature importance

### Classifying features using the Random Forest Classifier to sort the most important
"""

#Using RandomForestClassifier
model = RandomForestClassifier()
model.fit(clean_data, y)  # Train the model

# Access feature importances
feature_importances = model.feature_importances_

#Visualizing the feature importance
plt.figure(figsize=(10, 6))
plt.bar(range(len(feature_importances)), feature_importances, tick_label=clean_data.columns)
plt.xlabel('Features')
plt.ylabel('Feature Importance')
plt.title('Feature Importance in RandomForest Model')
plt.xticks(rotation='vertical')
plt.show()

#Ranking the features based on the results above
feature_names = clean_data.columns # Replace with your feature names

# Sort feature importance in descending order
indices = np.argsort(feature_importances)[::-1]

# Print feature ranking
print("Feature ranking based on Random Forest Classifier:")
for f in range(len(feature_names)):
    print(f"{f + 1}. {feature_names[indices[f]]}: {feature_importances[indices[f]]}")

"""###Analysing feature importance based on correlation matrix with target variable 'overall'"""

correlation = clean_data.corrwith(y).abs()
sorted_correlation = correlation.sort_values(ascending=False)
sorted_correlation

"""###Selecting top 35 features from correlation to create a subset of the data based on their relevance to the overal rating"""

N = 35
top_features = sorted_correlation.index[:N]
feature_subset = clean_data[top_features]
feature_subset

"""#

###Selecting the best 10 features using PCA from the model created from the correlation matrix
"""

from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_regression
import pandas as pd

# Perform feature selection to determine the best features (e.g., using SelectKBest with f_regression)
k_best = SelectKBest(score_func=f_regression, k=10)
best_features = k_best.fit_transform(feature_subset, y)

# Get the column names of the best features
best_feature_indices = k_best.get_support(indices=True)
best_feature_names = feature_subset.columns[best_feature_indices]

# Initialize PCA without specifying the number of components
pca = PCA()

# Fit the PCA model to your data
pca.fit(best_features)

# Get the explained variance of each principal component
explained_variance = pca.explained_variance_ratio_

# Sort the feature names based on the explained variance (highest variance first)
sorted_feature_names = [name for _, name in sorted(zip(explained_variance, best_feature_names), reverse=True)]

# Select the top N feature names
top_n = 10
selected_feature_names = sorted_feature_names[:top_n]
newX = feature_subset[selected_feature_names]
newX

"""An "Explained Variance" of 100.00% in PCA means that all the variation present in the original data is accounted for by the selected principal components. Essentially, none of the data's variability remains unexplained, and you have retained all the important information in a more compact form through PCA."""

explained_variance = pca.explained_variance_ratio_
print(f"Explained Variance: {explained_variance.sum() * 100:.2f}%")

"""### Standardising the independent variables and inputting the proper column names and formats"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

scaled_features=scaler.fit_transform(newX)

scaled_df = pd.DataFrame(scaled_features, columns=newX.columns)
X= scaled_df

X

"""# 3. Training and Testing the model

# Training the model with cross validation
"""

from sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import accuracy_score, f1_score, mean_squared_error, mean_squared_error, mean_absolute_error
from sklearn.model_selection import cross_val_score, RandomizedSearchCV
from xgboost import XGBRegressor

# Models
models = [
    ('Random Forest', RandomForestRegressor()),
    ('XGBoost', XGBRegressor()),
    ('Gradient Boosting', GradientBoostingRegressor())
]

# Initialize an ensemble model
ensemble_model = VotingRegressor(models)
# Define the number of folds for cross-validation
num_folds = 5

"""Random Forest:

    Cross-Validation Scores: Range from -2.15 to -4.65, reflecting model performance in different data subsets.
    Mean Score: Approximately -3.58, indicating the average prediction error across the folds.

XGBoost:

    Cross-Validation Scores: Range from -2.17 to -4.65, showing model consistency in different data subsets.
    Mean Score: Approximately -3.58, representing the overall performance and prediction accuracy.

Gradient Boosting:

    Cross-Validation Scores: Range from -2.18 to -4.63, showcasing model performance across various data subsets.
    Mean Score: Approximately -3.57, summarizing the average prediction error and consistency.
"""

# Iterate through each model
for model_name, model in models:
    # Perform cross-validation
    scores = cross_val_score(ensemble_model, X, y, cv=num_folds)

    print(f"Model: {model_name}")
    print("Cross-Validation Scores:")
    print(scores)
    print(f"Mean Score: {scores.mean()}\n")

# Train the ensemble model on the entire dataset
ensemble_model.fit(X, y)

"""# 4. Evaluation

Random Forest:

    RMSE (Root Mean Squared Error): Measures the average magnitude of prediction errors. An RMSE of 4.33 indicates, on average, predictions are off by approximately 4.33 units.
    MAE (Mean Absolute Error): Represents the average absolute difference between predictions and actual values. An MAE of 3.71 signifies an average absolute prediction error of about 3.71 units.

XGBoost:

    RMSE: Indicates an average prediction error of 3.91 units, making it more accurate on average compared to Random Forest.
    MAE: Suggests that, on average, predictions deviate from actual values by approximately 3.31 units.

Gradient Boosting:

    RMSE: Represents an average error of approximately 4.12 units, making it less accurate than XGBoost but better than Random Forest.
    MAE: Suggests an average absolute prediction error of about 3.54 units, which falls between XGBoost and Random Forest in terms of accuracy.

In summary, lower values for both RMSE and MAE indicate more accurate model predictions. Among the three models, XGBoost generally performs the best, followed by Gradient Boosting, and Random Forest has the highest average prediction errors.
"""

# Cross-validation and model evaluation (MAE and RMSE)
for name, model in models:
    rmse_scores = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error'))
    mae_scores = -cross_val_score(model, X, y, scoring='neg_mean_absolute_error')
    print(f"{name} - Cross-Validation RMSE: {np.mean(rmse_scores):.2f} (±{np.std(rmse_scores):.2f})")
    print(f"{name} - Cross-Validation MAE: {np.mean(mae_scores):.2f} (±{np.std(mae_scores):.2f})")

"""Entering the various hyperparameter to be tested for the model"""

#Models and their respective hyperparameter grids
models = {
    'Random Forest': (RandomForestRegressor(), {
        'n_estimators': [100, 200, 300],
        'max_depth': [None, 10, 20, 30],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }),
    'XGBoost': (XGBRegressor(), {
        'n_estimators': [100, 200, 300],
        'max_depth': [3, 4, 5],
        'learning_rate': [0.01, 0.1, 0.2]
    }),
    'Gradient Boosting': (GradientBoostingRegressor(), {
        'n_estimators': [100, 200, 300],
        'max_depth': [3, 4, 5],
        'learning_rate': [0.01, 0.1, 0.2]
    })
}

"""Chosed grid search over randomised search since its more efficient and less memory intensive. Below the code chunk, there are best hyperparameter outputted from fine tuning"""

# Randomized search and model selection
best_models = {}
for name, (model, param_dist) in models.items():
    random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=10, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
    random_search.fit(X, y)
    best_models[name] = random_search.best_estimator_
    print(f"Best {name} Model Parameters: {random_search.best_params_}")

# Create an ensemble of the best-tuned models
ensemble_regressor = VotingRegressor(estimators=[
    ('Random Forest', best_models['Random Forest']),
    ('XGBoost', best_models['XGBoost']),
    ('Gradient Boosting', best_models['Gradient Boosting'])
])

# Fit the ensemble regressor to the data
ensemble_regressor.fit(X, y)

"""# 5. Testing with new data set

### Removing useless variables (Those with 30% missing values)

Dropped excatly the same columns as in the training dataset to ensure standadised data for comparism
"""

#Loading the datasets to be used (both training and testing)
testing_data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/A 2023/players_22.csv')
data2= testing_data.copy()

testing_data = testing_data[['movement_reactions','overall', 'mentality_composure', 'passing', 'potential', 'dribbling','power_shot_power','physic','mentality_vision','attacking_short_passing', 'skill_long_passing']]
testing_data

threshold = 0.30 *len(testing_data)

testing_data = testing_data.dropna(thresh=threshold, axis =1)

"""### Splitting numerical and categorical"""

Ytest = testing_data['overall']
testing_data.drop('overall', axis=1,inplace=True)

numerical_col = testing_data.select_dtypes(include=['int64', 'float64']).columns
categorical_col = testing_data.select_dtypes(include=['object']).columns

categorical_var_test = testing_data[categorical_col]
numerical_var_test = testing_data[numerical_col]

numerical_var_test

"""### KNN imputation of numerical variables"""

knn_imputer = KNNImputer(n_neighbors=8)
numerical_var_imputed_2 = knn_imputer.fit_transform(numerical_var_test)
numerical_columns = list(numerical_var_test.columns)
numerical_var_imputed_2 = pd.DataFrame(numerical_var_imputed_2, columns=numerical_col)

"""### Joining back the clean numerical and categorical datasets"""

Xtest = numerical_var_imputed_2
Xtest

"""Standardising the independent variables"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

scaled_features=scaler.fit_transform(Xtest)

columns_order = ['movement_reactions', 'mentality_composure', 'passing', 'potential', 'dribbling','power_shot_power','physic','mentality_vision','attacking_short_passing', 'skill_long_passing']

# 2. Create a new DataFrame with the extracted columns
newX = pd.DataFrame(scaled_features, columns=columns_order)

newX

"""Evaluate model using player_22 test data

**Mean Absolute Error (MAE)**
    An MAE of approximately 1.41 indicates that, on average, the model's predictions differ from actual values by about 1.41 units. Lower MAE values are generally better, and this value suggests that the model's predictions are reasonably close to the actual values.

  **Mean Squared Error (MSE)**
    The MSE of around 3.88 is a measure of the average squared prediction errors. Lower MSE values are better, and this value suggests that the model's predictions have relatively small squared differences from the actual values.

  **Root Mean Squared Error (RMSE)**
    An RMSE of roughly 1.97, which is the square root of the MSE, signifies the typical magnitude of prediction errors. Lower RMSE values are better, and this value suggests that, on average, the model's predictions are close to the actual values.

 ** R-squared (R2)**
    An R2 of approximately 0.92 indicates that the model accounts for about 92% of the variance in the data. Higher R2 values are better, and this value suggests that the model is explaining a significant portion of the data's variability.
"""

from sklearn.metrics import r2_score
predictions = ensemble_regressor.predict(newX)
mae = mean_absolute_error(Ytest, predictions)
mse = mean_squared_error(Ytest, predictions)
rmse = np.sqrt(mse)
r2 = r2_score(Ytest, predictions)

print(f"Mean Absolute Error (MAE): {mae}")
print(f"Mean Squared Error (MSE): {mse}")
print(f"Root Mean Squared Error (RMSE): {rmse}")
print(f"R-squared (R2): {r2}")

"""# 6. Deployment"""

'''#Saving the model in a pickle file
import pickle
filename = 'trained_model.pkl'
pickle.dump(ensemble_regressor, open(filename,'wb'))'''
import joblib

# Saving the model
joblib.dump(ensemble_regressor, 'trained_model.pkl')